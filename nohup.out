WARNING ⚠️ no model scale passed. Assuming scale='n'.
Traceback (most recent call last):
  File "/root/Anti-UAV/tools/train.py", line 7, in <module>
    model = YOLO(r'/root/Anti-UAV/ultralytics/cfg/models/11/yolo11-sam.yaml', task="detect")  # 不使用预训练权重训练 | detect, segment, classify, pose, obb
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/models/yolo/model.py", line 83, in __init__
    super().__init__(model=model, task=task, verbose=verbose)
  File "/root/Anti-UAV/ultralytics/engine/model.py", line 151, in __init__
    self._new(model, task=task, verbose=verbose)
  File "/root/Anti-UAV/ultralytics/engine/model.py", line 263, in _new
    self.model = (model or self._smart_load("model"))(cfg_dict, verbose=verbose and RANK == -1)  # build model
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 399, in __init__
    self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 1693, in parse_model
    m_ = torch.nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module
                                                                          ^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/Addmodules/SAMBackbone.py", line 26, in __init__
    self.sam2.load_state_dict(state_dict)
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 2624, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Sam2Model:
	Missing key(s) in state_dict: "no_memory_embedding", "shared_image_embedding.positional_embedding", "vision_encoder.backbone.pos_embed", "vision_encoder.backbone.pos_embed_window", "vision_encoder.backbone.patch_embed.projection.weight", "vision_encoder.backbone.patch_embed.projection.bias", "vision_encoder.backbone.blocks.0.layer_norm1.weight", "vision_encoder.backbone.blocks.0.layer_norm1.bias", "vision_encoder.backbone.blocks.0.attn.qkv.weight", "vision_encoder.backbone.blocks.0.attn.qkv.bias", "vision_encoder.backbone.blocks.0.attn.proj.weight", "vision_encoder.backbone.blocks.0.attn.proj.bias", "vision_encoder.backbone.blocks.0.layer_norm2.weight", "vision_encoder.backbone.blocks.0.layer_norm2.bias", "vision_encoder.backbone.blocks.0.mlp.proj_in.weight", "vision_encoder.backbone.blocks.0.mlp.proj_in.bias", "vision_encoder.backbone.blocks.0.mlp.proj_out.weight", "vision_encoder.backbone.blocks.0.mlp.proj_out.bias", "vision_encoder.backbone.blocks.1.layer_norm1.weight", "vision_encoder.backbone.blocks.1.layer_norm1.bias", "vision_encoder.backbone.blocks.1.attn.qkv.weight", "vision_encoder.backbone.blocks.1.attn.qkv.bias", "vision_encoder.backbone.blocks.1.attn.proj.weight", "vision_encoder.backbone.blocks.1.attn.proj.bias", "vision_encoder.backbone.blocks.1.layer_norm2.weight", "vision_encoder.backbone.blocks.1.layer_norm2.bias", "vision_encoder.backbone.blocks.1.mlp.proj_in.weight", "vision_encoder.backbone.blocks.1.mlp.proj_in.bias", "vision_encoder.backbone.blocks.1.mlp.proj_out.weight", "vision_encoder.backbone.blocks.1.mlp.proj_out.bias", "vision_encoder.backbone.blocks.1.proj.weight", "vision_encoder.backbone.blocks.1.proj.bias", "vision_encoder.backbone.blocks.2.layer_norm1.weight", "vision_encoder.backbone.blocks.2.layer_norm1.bias", "vision_encoder.backbone.blocks.2.attn.qkv.weight", "vision_encoder.backbone.blocks.2.attn.qkv.bias", "vision_encoder.backbone.blocks.2.attn.proj.weight", "vision_encoder.backbone.blocks.2.attn.proj.bias", "vision_encoder.backbone.blocks.2.layer_norm2.weight", "vision_encoder.backbone.blocks.2.layer_norm2.bias", "vision_encoder.backbone.blocks.2.mlp.proj_in.weight", "vision_encoder.backbone.blocks.2.mlp.proj_in.bias", "vision_encoder.backbone.blocks.2.mlp.proj_out.weight", "vision_encoder.backbone.blocks.2.mlp.proj_out.bias", "vision_encoder.backbone.blocks.3.layer_norm1.weight", "vision_encoder.backbone.blocks.3.layer_norm1.bias", "vision_encoder.backbone.blocks.3.attn.qkv.weight", "vision_encoder.backbone.blocks.3.attn.qkv.bias", "vision_encoder.backbone.blocks.3.attn.proj.weight", "vision_encoder.backbone.blocks.3.attn.proj.bias", "vision_encoder.backbone.blocks.3.layer_norm2.weight", "vision_encoder.backbone.blocks.3.layer_norm2.bias", "vision_encoder.backbone.blocks.3.mlp.proj_in.weight", "vision_encoder.backbone.blocks.3.mlp.proj_in.bias", "vision_encoder.backbone.blocks.3.mlp.proj_out.weight", "vision_encoder.backbone.blocks.3.mlp.proj_out.bias", "vision_encoder.backbone.blocks.3.proj.weight", "vision_encoder.backbone.blocks.3.proj.bias", "vision_encoder.backbone.blocks.4.layer_norm1.weight", "vision_encoder.backbone.blocks.4.layer_norm1.bias", "vision_encoder.backbone.blocks.4.attn.qkv.weight", "vision_encoder.backbone.blocks.4.attn.qkv.bias", "vision_encoder.backbone.blocks.4.attn.proj.weight", "vision_encoder.backbone.blocks.4.attn.proj.bias", "vision_encoder.backbone.blocks.4.layer_norm2.weight", "vision_encoder.backbone.blocks.4.layer_norm2.bias", "vision_encoder.backbone.blocks.4.mlp.proj_in.weight", "vision_encoder.backbone.blocks.4.mlp.proj_in.bias", "vision_encoder.backbone.blocks.4.mlp.proj_out.weight", "vision_encoder.backbone.blocks.4.mlp.proj_out.bias", "vision_encoder.backbone.blocks.5.layer_norm1.weight", "vision_encoder.backbone.blocks.5.layer_norm1.bias", "vision_encoder.backbone.blocks.5.attn.qkv.weight", "vision_encoder.backbone.blocks.5.attn.qkv.bias", "vision_encoder.backbone.blocks.5.attn.proj.weight", "vision_encoder.backbone.blocks.5.attn.proj.bias", "vision_encoder.backbone.blocks.5.layer_norm2.weight", "vision_encoder.backbone.blocks.5.layer_norm2.bias", "vision_encoder.backbone.blocks.5.mlp.proj_in.weight", "vision_encoder.backbone.blocks.5.mlp.proj_in.bias", "vision_encoder.backbone.blocks.5.mlp.proj_out.weight", "vision_encoder.backbone.blocks.5.mlp.proj_out.bias", "vision_encoder.backbone.blocks.6.layer_norm1.weight", "vision_encoder.backbone.blocks.6.layer_norm1.bias", "vision_encoder.backbone.blocks.6.attn.qkv.weight", "vision_encoder.backbone.blocks.6.attn.qkv.bias", "vision_encoder.backbone.blocks.6.attn.proj.weight", "vision_encoder.backbone.blocks.6.attn.proj.bias", "vision_encoder.backbone.blocks.6.layer_norm2.weight", "vision_encoder.backbone.blocks.6.layer_norm2.bias", "vision_encoder.backbone.blocks.6.mlp.proj_in.weight", "vision_encoder.backbone.blocks.6.mlp.proj_in.bias", "vision_encoder.backbone.blocks.6.mlp.proj_out.weight", "vision_encoder.backbone.blocks.6.mlp.proj_out.bias", "vision_encoder.backbone.blocks.7.layer_norm1.weight", "vision_encoder.backbone.blocks.7.layer_norm1.bias", "vision_encoder.backbone.blocks.7.attn.qkv.weight", "vision_encoder.backbone.blocks.7.attn.qkv.bias", "vision_encoder.backbone.blocks.7.attn.proj.weight", "vision_encoder.backbone.blocks.7.attn.proj.bias", "vision_encoder.backbone.blocks.7.layer_norm2.weight", "vision_encoder.backbone.blocks.7.layer_norm2.bias", "vision_encoder.backbone.blocks.7.mlp.proj_in.weight", "vision_encoder.backbone.blocks.7.mlp.proj_in.bias", "vision_encoder.backbone.blocks.7.mlp.proj_out.weight", "vision_encoder.backbone.blocks.7.mlp.proj_out.bias", "vision_encoder.backbone.blocks.8.layer_norm1.weight", "vision_encoder.backbone.blocks.8.layer_norm1.bias", "vision_encoder.backbone.blocks.8.attn.qkv.weight", "vision_encoder.backbone.blocks.8.attn.qkv.bias", "vision_encoder.backbone.blocks.8.attn.proj.weight", "vision_encoder.backbone.blocks.8.attn.proj.bias", "vision_encoder.backbone.blocks.8.layer_norm2.weight", "vision_encoder.backbone.blocks.8.layer_norm2.bias", "vision_encoder.backbone.blocks.8.mlp.proj_in.weight", "vision_encoder.backbone.blocks.8.mlp.proj_in.bias", "vision_encoder.backbone.blocks.8.mlp.proj_out.weight", "vision_encoder.backbone.blocks.8.mlp.proj_out.bias", "vision_encoder.backbone.blocks.9.layer_norm1.weight", "vision_encoder.backbone.blocks.9.layer_norm1.bias", "vision_encoder.backbone.blocks.9.attn.qkv.weight", "vision_encoder.backbone.blocks.9.attn.qkv.bias", "vision_encoder.backbone.blocks.9.attn.proj.weight", "vision_encoder.backbone.blocks.9.attn.proj.bias", "vision_encoder.backbone.blocks.9.layer_norm2.weight", "vision_encoder.backbone.blocks.9.layer_norm2.bias", "vision_encoder.backbone.blocks.9.mlp.proj_in.weight", "vision_encoder.backbone.blocks.9.mlp.proj_in.bias", "vision_encoder.backbone.blocks.9.mlp.proj_out.weight", "vision_encoder.backbone.blocks.9.mlp.proj_out.bias", "vision_encoder.backbone.blocks.10.layer_norm1.weight", "vision_encoder.backbone.blocks.10.layer_norm1.bias", "vision_encoder.backbone.blocks.10.attn.qkv.weight", "vision_encoder.backbone.blocks.10.attn.qkv.bias", "vision_encoder.backbone.blocks.10.attn.proj.weight", "vision_encoder.backbone.blocks.10.attn.proj.bias", "vision_encoder.backbone.blocks.10.layer_norm2.weight", "vision_encoder.backbone.blocks.10.layer_norm2.bias", "vision_encoder.backbone.blocks.10.mlp.proj_in.weight", "vision_encoder.backbone.blocks.10.mlp.proj_in.bias", "vision_encoder.backbone.blocks.10.mlp.proj_out.weight", "vision_encoder.backbone.blocks.10.mlp.proj_out.bias", "vision_encoder.backbone.blocks.10.proj.weight", "vision_encoder.backbone.blocks.10.proj.bias", "vision_encoder.backbone.blocks.11.layer_norm1.weight", "vision_encoder.backbone.blocks.11.layer_norm1.bias", "vision_encoder.backbone.blocks.11.attn.qkv.weight", "vision_encoder.backbone.blocks.11.attn.qkv.bias", "vision_encoder.backbone.blocks.11.attn.proj.weight", "vision_encoder.backbone.blocks.11.attn.proj.bias", "vision_encoder.backbone.blocks.11.layer_norm2.weight", "vision_encoder.backbone.blocks.11.layer_norm2.bias", "vision_encoder.backbone.blocks.11.mlp.proj_in.weight", "vision_encoder.backbone.blocks.11.mlp.proj_in.bias", "vision_encoder.backbone.blocks.11.mlp.proj_out.weight", "vision_encoder.backbone.blocks.11.mlp.proj_out.bias", "vision_encoder.neck.convs.0.weight", "vision_encoder.neck.convs.0.bias", "vision_encoder.neck.convs.1.weight", "vision_encoder.neck.convs.1.bias", "vision_encoder.neck.convs.2.weight", "vision_encoder.neck.convs.2.bias", "vision_encoder.neck.convs.3.weight", "vision_encoder.neck.convs.3.bias", "prompt_encoder.shared_embedding.positional_embedding", "prompt_encoder.mask_embed.conv1.weight", "prompt_encoder.mask_embed.conv1.bias", "prompt_encoder.mask_embed.conv2.weight", "prompt_encoder.mask_embed.conv2.bias", "prompt_encoder.mask_embed.conv3.weight", "prompt_encoder.mask_embed.conv3.bias", "prompt_encoder.mask_embed.layer_norm1.weight", "prompt_encoder.mask_embed.layer_norm1.bias", "prompt_encoder.mask_embed.layer_norm2.weight", "prompt_encoder.mask_embed.layer_norm2.bias", "prompt_encoder.no_mask_embed.weight", "prompt_encoder.point_embed.weight", "prompt_encoder.not_a_point_embed.weight", "mask_decoder.iou_token.weight", "mask_decoder.mask_tokens.weight", "mask_decoder.transformer.layers.0.self_attn.q_proj.weight", "mask_decoder.transformer.layers.0.self_attn.q_proj.bias", "mask_decoder.transformer.layers.0.self_attn.k_proj.weight", "mask_decoder.transformer.layers.0.self_attn.k_proj.bias", "mask_decoder.transformer.layers.0.self_attn.v_proj.weight", "mask_decoder.transformer.layers.0.self_attn.v_proj.bias", "mask_decoder.transformer.layers.0.self_attn.o_proj.weight", "mask_decoder.transformer.layers.0.self_attn.o_proj.bias", "mask_decoder.transformer.layers.0.layer_norm1.weight", "mask_decoder.transformer.layers.0.layer_norm1.bias", "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight", "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias", "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight", "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias", "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight", "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias", "mask_decoder.transformer.layers.0.cross_attn_token_to_image.o_proj.weight", "mask_decoder.transformer.layers.0.cross_attn_token_to_image.o_proj.bias", "mask_decoder.transformer.layers.0.layer_norm2.weight", "mask_decoder.transformer.layers.0.layer_norm2.bias", "mask_decoder.transformer.layers.0.mlp.proj_in.weight", "mask_decoder.transformer.layers.0.mlp.proj_in.bias", "mask_decoder.transformer.layers.0.mlp.proj_out.weight", "mask_decoder.transformer.layers.0.mlp.proj_out.bias", "mask_decoder.transformer.layers.0.layer_norm3.weight", "mask_decoder.transformer.layers.0.layer_norm3.bias", "mask_decoder.transformer.layers.0.layer_norm4.weight", "mask_decoder.transformer.layers.0.layer_norm4.bias", "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight", "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias", "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight", "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias", "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight", "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias", "mask_decoder.transformer.layers.0.cross_attn_image_to_token.o_proj.weight", "mask_decoder.transformer.layers.0.cross_attn_image_to_token.o_proj.bias", "mask_decoder.transformer.layers.1.self_attn.q_proj.weight", "mask_decoder.transformer.layers.1.self_attn.q_proj.bias", "mask_decoder.transformer.layers.1.self_attn.k_proj.weight", "mask_decoder.transformer.layers.1.self_attn.k_proj.bias", "mask_decoder.transformer.layers.1.self_attn.v_proj.weight", "mask_decoder.transformer.layers.1.self_attn.v_proj.bias", "mask_decoder.transformer.layers.1.self_attn.o_proj.weight", "mask_decoder.transformer.layers.1.self_attn.o_proj.bias", "mask_decoder.transformer.layers.1.layer_norm1.weight", "mask_decoder.transformer.layers.1.layer_norm1.bias", "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight", "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias", "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight", "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias", "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight", "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias", "mask_decoder.transformer.layers.1.cross_attn_token_to_image.o_proj.weight", "mask_decoder.transformer.layers.1.cross_attn_token_to_image.o_proj.bias", "mask_decoder.transformer.layers.1.layer_norm2.weight", "mask_decoder.transformer.layers.1.layer_norm2.bias", "mask_decoder.transformer.layers.1.mlp.proj_in.weight", "mask_decoder.transformer.layers.1.mlp.proj_in.bias", "mask_decoder.transformer.layers.1.mlp.proj_out.weight", "mask_decoder.transformer.layers.1.mlp.proj_out.bias", "mask_decoder.transformer.layers.1.layer_norm3.weight", "mask_decoder.transformer.layers.1.layer_norm3.bias", "mask_decoder.transformer.layers.1.layer_norm4.weight", "mask_decoder.transformer.layers.1.layer_norm4.bias", "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight", "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias", "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight", "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias", "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight", "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias", "mask_decoder.transformer.layers.1.cross_attn_image_to_token.o_proj.weight", "mask_decoder.transformer.layers.1.cross_attn_image_to_token.o_proj.bias", "mask_decoder.transformer.final_attn_token_to_image.q_proj.weight", "mask_decoder.transformer.final_attn_token_to_image.q_proj.bias", "mask_decoder.transformer.final_attn_token_to_image.k_proj.weight", "mask_decoder.transformer.final_attn_token_to_image.k_proj.bias", "mask_decoder.transformer.final_attn_token_to_image.v_proj.weight", "mask_decoder.transformer.final_attn_token_to_image.v_proj.bias", "mask_decoder.transformer.final_attn_token_to_image.o_proj.weight", "mask_decoder.transformer.final_attn_token_to_image.o_proj.bias", "mask_decoder.transformer.layer_norm_final_attn.weight", "mask_decoder.transformer.layer_norm_final_attn.bias", "mask_decoder.upscale_conv1.weight", "mask_decoder.upscale_conv1.bias", "mask_decoder.upscale_conv2.weight", "mask_decoder.upscale_conv2.bias", "mask_decoder.upscale_layer_norm.weight", "mask_decoder.upscale_layer_norm.bias", "mask_decoder.output_hypernetworks_mlps.0.proj_in.weight", "mask_decoder.output_hypernetworks_mlps.0.proj_in.bias", "mask_decoder.output_hypernetworks_mlps.0.proj_out.weight", "mask_decoder.output_hypernetworks_mlps.0.proj_out.bias", "mask_decoder.output_hypernetworks_mlps.0.layers.0.weight", "mask_decoder.output_hypernetworks_mlps.0.layers.0.bias", "mask_decoder.output_hypernetworks_mlps.1.proj_in.weight", "mask_decoder.output_hypernetworks_mlps.1.proj_in.bias", "mask_decoder.output_hypernetworks_mlps.1.proj_out.weight", "mask_decoder.output_hypernetworks_mlps.1.proj_out.bias", "mask_decoder.output_hypernetworks_mlps.1.layers.0.weight", "mask_decoder.output_hypernetworks_mlps.1.layers.0.bias", "mask_decoder.output_hypernetworks_mlps.2.proj_in.weight", "mask_decoder.output_hypernetworks_mlps.2.proj_in.bias", "mask_decoder.output_hypernetworks_mlps.2.proj_out.weight", "mask_decoder.output_hypernetworks_mlps.2.proj_out.bias", "mask_decoder.output_hypernetworks_mlps.2.layers.0.weight", "mask_decoder.output_hypernetworks_mlps.2.layers.0.bias", "mask_decoder.output_hypernetworks_mlps.3.proj_in.weight", "mask_decoder.output_hypernetworks_mlps.3.proj_in.bias", "mask_decoder.output_hypernetworks_mlps.3.proj_out.weight", "mask_decoder.output_hypernetworks_mlps.3.proj_out.bias", "mask_decoder.output_hypernetworks_mlps.3.layers.0.weight", "mask_decoder.output_hypernetworks_mlps.3.layers.0.bias", "mask_decoder.iou_prediction_head.proj_in.weight", "mask_decoder.iou_prediction_head.proj_in.bias", "mask_decoder.iou_prediction_head.proj_out.weight", "mask_decoder.iou_prediction_head.proj_out.bias", "mask_decoder.iou_prediction_head.layers.0.weight", "mask_decoder.iou_prediction_head.layers.0.bias", "mask_decoder.conv_s0.weight", "mask_decoder.conv_s0.bias", "mask_decoder.conv_s1.weight", "mask_decoder.conv_s1.bias", "mask_decoder.obj_score_token.weight", "mask_decoder.pred_obj_score_head.proj_in.weight", "mask_decoder.pred_obj_score_head.proj_in.bias", "mask_decoder.pred_obj_score_head.proj_out.weight", "mask_decoder.pred_obj_score_head.proj_out.bias", "mask_decoder.pred_obj_score_head.layers.0.weight", "mask_decoder.pred_obj_score_head.layers.0.bias". 
	Unexpected key(s) in state_dict: "model". 
WARNING ⚠️ no model scale passed. Assuming scale='n'.
Traceback (most recent call last):
  File "/root/Anti-UAV/tools/train.py", line 7, in <module>
    model = YOLO(r'/root/Anti-UAV/ultralytics/cfg/models/11/yolo11-sam.yaml', task="detect")  # 不使用预训练权重训练 | detect, segment, classify, pose, obb
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/models/yolo/model.py", line 83, in __init__
    super().__init__(model=model, task=task, verbose=verbose)
  File "/root/Anti-UAV/ultralytics/engine/model.py", line 151, in __init__
    self._new(model, task=task, verbose=verbose)
  File "/root/Anti-UAV/ultralytics/engine/model.py", line 263, in _new
    self.model = (model or self._smart_load("model"))(cfg_dict, verbose=verbose and RANK == -1)  # build model
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 399, in __init__
    self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 1693, in parse_model
    m_ = torch.nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module
                                                                          ^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/Addmodules/SAMBackbone.py", line 46, in __init__
    self.encoder = self.sam2.image_encoder
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1962, in __getattr__
    raise AttributeError(
AttributeError: 'Sam2Model' object has no attribute 'image_encoder'. Did you mean: 'vision_encoder'?
WARNING ⚠️ no model scale passed. Assuming scale='n'.
Traceback (most recent call last):
  File "/root/Anti-UAV/tools/train.py", line 7, in <module>
    model = YOLO(r'/root/Anti-UAV/ultralytics/cfg/models/11/yolo11-sam.yaml', task="detect")  # 不使用预训练权重训练 | detect, segment, classify, pose, obb
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/models/yolo/model.py", line 83, in __init__
    super().__init__(model=model, task=task, verbose=verbose)
  File "/root/Anti-UAV/ultralytics/engine/model.py", line 151, in __init__
    self._new(model, task=task, verbose=verbose)
  File "/root/Anti-UAV/ultralytics/engine/model.py", line 263, in _new
    self.model = (model or self._smart_load("model"))(cfg_dict, verbose=verbose and RANK == -1)  # build model
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 418, in __init__
    m.stride = torch.tensor([s / x.shape[-2] for x in _forward(torch.zeros(1, ch, s, s))])  # forward
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 414, in _forward
    return self.forward(x)[0] if isinstance(m, (Segment, YOLOESegment, Pose, OBB)) else self.forward(x)
                                                                                        ^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 139, in forward
    return self.predict(x, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 157, in predict
    return self._predict_once(x, profile, visualize, embed)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 180, in _predict_once
    x = m(x)  # run
        ^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/modules/conv.py", line 81, in forward
    return self.act(self.bn(self.conv(x)))
                            ^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
TypeError: conv2d() received an invalid combination of arguments - got (Sam2VisionEncoderOutput, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: (!Sam2VisionEncoderOutput!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = "valid", tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: (!Sam2VisionEncoderOutput!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)

WARNING ⚠️ no model scale passed. Assuming scale='n'.
Traceback (most recent call last):
  File "/root/Anti-UAV/tools/train.py", line 7, in <module>
    model = YOLO(r'/root/Anti-UAV/ultralytics/cfg/models/11/yolo11-sam.yaml', task="detect")  # 不使用预训练权重训练 | detect, segment, classify, pose, obb
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/models/yolo/model.py", line 83, in __init__
    super().__init__(model=model, task=task, verbose=verbose)
  File "/root/Anti-UAV/ultralytics/engine/model.py", line 151, in __init__
    self._new(model, task=task, verbose=verbose)
  File "/root/Anti-UAV/ultralytics/engine/model.py", line 263, in _new
    self.model = (model or self._smart_load("model"))(cfg_dict, verbose=verbose and RANK == -1)  # build model
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 418, in __init__
    m.stride = torch.tensor([s / x.shape[-2] for x in _forward(torch.zeros(1, ch, s, s))])  # forward
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 414, in _forward
    return self.forward(x)[0] if isinstance(m, (Segment, YOLOESegment, Pose, OBB)) else self.forward(x)
                                                                                        ^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 139, in forward
    return self.predict(x, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 157, in predict
    return self._predict_once(x, profile, visualize, embed)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/tasks.py", line 180, in _predict_once
    x = m(x)  # run
        ^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/Anti-UAV/ultralytics/nn/modules/conv.py", line 81, in forward
    return self.act(self.bn(self.conv(x)))
                            ^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 548, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 543, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [16, 3, 3, 3], expected input[1, 8, 8, 768] to have 3 channels, but got 8 channels instead
